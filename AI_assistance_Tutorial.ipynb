{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nU_OxkjsfUMv"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import bernoulli, multivariate_normal, norm\n",
    "from scipy.optimize import minimize\n",
    "import copy, os\n",
    "from tutorialObjs import World, UserModel, Assistant, crUser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Easy Navigation\n",
    "* [Code testing area](#testing)\n",
    "* [Tutorial starts here](#title)\n",
    "* [Introduction](#intro)\n",
    "* [1) The World](#world)\n",
    "* [2) The Assistant](#ai)\n",
    "    * [Task 1: Implement user model](#task1)\n",
    "    <!--     * [Task 1 part i](#task_11)\n",
    "        * [Task 1 part ii](#task_12)\n",
    "        * [Task 1 part iii](#task_13) -->\n",
    "    * [Task 2: Implement param inference](#task2)\n",
    "    <!--     * [Case study i: \"all coins display tails\"](#task_21)\n",
    "        * [Case study ii: at least \"1 coin display tails\" ](#task_22)\n",
    "        * [(Bonus) Case study iii: \"Exactly 1 tail\"  ](#task_23) -->\n",
    "* [3) The AI-user interaction](#intxn)\n",
    "    * [Task 3: Implement interaction loop](#task3)\n",
    "    <!--     * [Task 3 part i](#task_31)\n",
    "        * [(Optional) Task 3 part ii](#task_32)\n",
    "        * [Task 3 part iii](#task_33)\n",
    "            * [Plot by mean/SD value](#task_33_abs)\n",
    "            * [Plot by RMSE](#task_33_rmse) -->\n",
    "* [Conclusions](#analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing area <a class=\"anchor\" id=\"testing\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpful debug references\n",
    "\n",
    "# from tutorialObjs import Env\n",
    "\n",
    "# task = World()\n",
    "# testenv = Env(task,UserModel(dict(user_params=[1,1,1])))\n",
    "# testenv.path_tuples\n",
    "\n",
    "# # Pick random path\n",
    "# myAssistant = Assistant()\n",
    "# myAssistant.observe(task)\n",
    "# myAssistant.take_action()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def run_task(policy):\n",
    "    task = World()\n",
    "    \n",
    "#     init_param = multivariate_normal.rvs(mean=np.zeros(3),cov=np.eye(3))\n",
    "    \n",
    "    anonUser = crUser()\n",
    "#     simUser = UserModel(dict(user_params=init_param))\n",
    "    myAssistant = Assistant(**dict(policy=policy))\n",
    "    \n",
    "    \n",
    "    interaction_count = 0\n",
    "    actions = dict(ai=[],user=[])\n",
    "    while not task.is_solved():\n",
    "        myAssistant.observe(task)\n",
    "        anonUser.observe(task)\n",
    "\n",
    "\n",
    "        # Step 1) The assistant gives a recommendation\n",
    "        ai_a = myAssistant.take_action()\n",
    "        task.step(ai_action=ai_a)\n",
    "        actions['ai'].append(str(ai_a))\n",
    "        \n",
    "        # Step 2) The user observes the action taken by the AI\n",
    "        anonUser.observe(task)\n",
    "        # Step 3) The user proposes counter journey or agrees to the recommendation\n",
    "        u_a = anonUser.take_action()\n",
    "        task.step(user_action=u_a)\n",
    "        actions['user'].append(str(u_a))\n",
    "\n",
    "        interaction_count += 1\n",
    "    \n",
    "    myAssistant.reset()\n",
    "    return interaction_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "count_history = dict(global_min = [],\n",
    "                     random = [])\n",
    "\n",
    "print(time.strftime(\"%H:%M:%S\", time.localtime()))\n",
    "for i in range(50):\n",
    "    \n",
    "    total_interactions = run_task(\"global_min\")\n",
    "    count_history[\"global_min\"].append(total_interactions)\n",
    "    \n",
    "    total_interactions = run_task(\"random\")\n",
    "    count_history[\"random\"].append(total_interactions)\n",
    "    \n",
    "print(time.strftime(\"%H:%M:%S\", time.localtime()))\n",
    "print(count_history)\n",
    "print([np.mean(c) for _,c in count_history.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative modelling, design and decision making with AI, Part II <a class=\"anchor\" id=\"title\"></a>\n",
    "\n",
    "Elena Shaw, Ali Khoshvishkaie, Sebastiaan De Peuter, Alex Hämäläinen, Samuel Kaski"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When doing research with Machine Learning, it's helpful to be able to separate the problems that have been solved from the problems you will need to solve for yourself. In our goal to help other researchers leverage the AI-assistance framework for their own research problems, we're developing this tutorial as a quick, hands-on introduction to the problem landscape. \n",
    "\n",
    "AI assistance encompasses any task, game, or problem involving a human and an AI agent that is tasked with helping the human navigate the dynamics of the task. The motivation is that the AI's help can make solving the task more efficient and less burdensome to the human. In practice, this can be achieved by helping the human by filtering out less efficient solutions, minimizing human error, steering focus away from distractions etc. \n",
    "\n",
    "Due to the introductory nature of this tutorial, we will make use of existing solutions without too much explanation of their details. Interested researchers can consider the additional resources provided to develop more depth on the relevant topics. In this tutorial, however, we focus on guiding you through implementing the key components of the framework you would need to specify according to your research problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction <a class=\"anchor\" id=\"intro\"></a>\n",
    "\n",
    "AI-assistance is fundamentally a sequential decision-making problem, in which the AI-assistant continually advises an action in the world and the user takes the ultimate decision based on the AI's advice, their understanding of the world, and personal goals. The AI-assistant needs to infer the goals of the user based on the actions they make, and as a result, learn to make better, more relevant recommendations to the user.\n",
    "\n",
    "The figure below illustrates the key elements that make up the AI-assistance framework. This tutorial will walk you through a simple implementation on a few of them.\n",
    "\n",
    "![ai-assistance-overview.png](./ai-assistance-overview.png)\n",
    "\n",
    "We'll introduce the key concepts of this framework through an illustrative journey planning problem. The task here is fundamentally a multi-objective optimization problem: an anonymous user wants to travel from city A to city B via a personalized journey itinerary catering to their travel preferences. \n",
    "\n",
    "**The Task:** Given that the AI assistant has never encountered this user before, how should it learn the optimal personalized recommendations for the user?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2IG720GsNINX"
   },
   "source": [
    "# 1) Defining the world <a class=\"anchor\" id=\"world\"></a>\n",
    "\n",
    "As a first step towards solving the problem of interest, let's begin by defining the environment. \n",
    "\n",
    "Consider there are 10 locations involved, 2 are accounted for based on where the journey should begin and end, then 8 are possible layover or transition locations situated in between. We arbitrarily define the start location to be number 7 and the end location to be 5. If you run the cell below which plots the task, you'll see that these two nodes have a darkened center to distinguish them from the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 753
    },
    "id": "5QVwizfMuDkx",
    "outputId": "3df37ad5-2f1a-4b70-e9d3-51b13f9011be"
   },
   "outputs": [],
   "source": [
    "np.random.seed(12345)\n",
    "\n",
    "task = World()\n",
    "print(\"Starting point:\", task.start)\n",
    "print(\"Destination:\", task.destination)\n",
    "plt.figure(figsize=(10,6))\n",
    "task.display_path()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these 10 locations are connected to others via several different transportation modes (e.g. train, ferry, plane, bus). Here, we'll let there be 5 different transportation mode, each represented by a graph $G_i=(V, E_i)$ where $V$ are the (set of) locations and $E_i$ are the edges connecting the locations via transportation mode $i$. The cell below visualizes these transportation mode graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different modes of transport will incur different costs. For example, shown in the cell below, taking mode 3 (e.g. a flight) between locations 1 and 2, $e^{12}_3 \\in E_3$, will be more expensive than traveling the same route via mode 2 (e.g. a bus), $e^{12}_2 \\in E_2$. The multiobjective trade off is that the more expensive journey via mode 3 will result in a shorter travel time than the journey via mode 2. \n",
    "\n",
    "For our task, each edge will have 3 properties which accumulate linearly if the user travels along that edge: travel distance, travel price, and travel time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode3_properties = task.look_up_cost((1,2,3))\n",
    "print(\"Travel 1 to 2 by mode 3...\")\n",
    "print(f'\\t Travel distance: {mode3_properties[0]} \\n'\n",
    "      f'\\t Travel price: {mode3_properties[1]} \\n'\n",
    "      f'\\t Travel time: {mode3_properties[2]} \\n')\n",
    "\n",
    "mode2_properties = task.look_up_cost((1,2,2))\n",
    "print(\"Travel 1 to 2 by mode 2...\")\n",
    "print(f'\\t Travel distance: {mode2_properties[0]} \\n'\n",
    "      f'\\t Travel price: {mode2_properties[1]} \\n'\n",
    "      f'\\t Travel time: {mode2_properties[2]} \\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) The Assistant <a class=\"anchor\" id=\"ai\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The User model\n",
    "\n",
    "AI-assistants are designed to help its users in a complementary way: provide advice and perspectives the user alone may not have access to. To do this effectively, the AI must understand the task in relation to the human; it must  define the problem as a task the *human* wishes to solve. It is not uncommon to frame the user task as a type of game, where there is a specific goal that the human agents is working toward. For AI-assistance, rather than being a single player game, the task can be modeled within an environment, where the user and AI agent cooperate to reach the goal. Often, these can be framed as decision problems: MDP, POMDPs, etc.\n",
    "\n",
    "In order for an AI assistant to be helpful to and cooperate with the human, it must have a way of learning about the human. Our framework does this with user models, where the AI learns and maintains a model of the human user. This is not unlike our own human behavior: when interacting with other humans, we develop mental models of the other person. And over time, by learning more about them, we leverage those models to infer other's goals and to predict how interactions with them may play out. Think about how you've undoubtedly held the door open for someone who was carrying something with both hands. This requires inferring that the person wanted to walk through the door in the first place, and predicting how the situation would change if you held the door open for them. We replicate this same concept for an AI agent through a user model. By maintaining a model of the user, the AI better plans and more efficiently cooperates with the user based on the inferred preferences and biases of the user.\n",
    "\n",
    "*(If you want a more complete overview of user models and how to define them, see slides _ to __ in the presentation deck.)*\n",
    "\n",
    "In our task, the user must navigate through a large space of potentially optimal decisions -- journey paths offering the best trade off according to individual preferences. This trade off is not known *a priori* and even if the relevant information to make this decision is available, it cannot be processed and filtered all at once. \n",
    "\n",
    "For this tutorial, we model this behavior under bounded rationality theory. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boundedly rational behavior:\n",
    "A bounded rational agent simulates human's limited cognitive ability to rationally process all available options at once. Here, due to the sheer number of available options, the user is unable to identify which journey from location 7 to 5 would be their most preferred. However, when presented with a limited subset of options, the user will have a strong preference based on the hidden preference parameter $\\boldsymbol{\\beta}^*$.\n",
    "\n",
    "To illustrate this, consider the world state below. The colors correspond to different transportation modes, but more importantly, note the solid versus dotted paths between locations 7 and 5. The dotted path is the journey itinerary suggested by the AI-assistance, while the solid path denotes the user's preferred itinerary. The cognitive limitation illustrated here is that the user's preferred path cannot and will not be any arbitrary (valid) path between 7 and 5. Instead, the user's actioned preference is limited to journeys that are close to the AI's original suggestion. \n",
    "\n",
    "*(The implementation details are not necessary to know for this tutorial, but if you feel you need to reference the details of our implementation, the relevant code can be found in the `self._find_alternatives()` method of the `User` object in `tutorialObjs.py`)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 699
    },
    "id": "zNdW3rsxhyUB",
    "outputId": "ebe0e7f9-fdda-474e-cd4f-19c4bc466a92"
   },
   "outputs": [],
   "source": [
    "# AI makes a recommendation\n",
    "ai_advice = [(7, 0, 2), (0, 3, 0), (3, 2, 1), (2, 1, 3), (1, 5, 4)]\n",
    "task.step(ai_action=ai_advice)\n",
    "\n",
    "# User responds with a counter proposal based on hidden preferences\n",
    "user_action = [(7, 0, 2), (0, 3, 0), (3, 2, 1), (2, 9, 2), (9, 5, 2)]\n",
    "task.step(user_action=user_action)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "task.display_path()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The assistant agent\n",
    "\n",
    "Given the components implemented above, you are now ready to implement your own AI-assistant. Below we've provided you with incomplete code for your AI-assistant. You will need to complete the two tasks below before your AI is functional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9LJahApAFvkP"
   },
   "source": [
    "## Task 1: Implement the user model <a class=\"anchor\" id=\"task1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the anonymous user who will have hidden preferences for how they weight the tradeoff among the available transportation modes for each $E_i$ of their journey. \n",
    "\n",
    "### Boltzmann model of rationality **[Insert information about Boltzmann Rationality here]**\n",
    "\n",
    "We will model this tradeoff using the Boltzmann model of rationality. Namely, that users are likely to take an action based on a policy of maximized utility across all possible actions $a_t$ at time $t$:\n",
    "$$\n",
    "\\text{Pr}(a_i|\\boldsymbol{\\beta}) \\propto \\exp \\Big \\{ \\boldsymbol{\\beta} \\cdot \\text{utility}(a_i) \\Big \\}\n",
    "$$\n",
    "where $\\boldsymbol{\\beta}$ is the user specific parameter vector.\n",
    "\n",
    "For ease of interpretation, we will leverage the inverse of this model in this tutorial, i.e. that the anonymous user will seek to minimize the total cost of the journey. The costs associated with each segment of the journey will play into the user's overall preference for any given journey, so you can interpret $\\boldsymbol{\\beta}$ as weight parameters on the edge properties. \n",
    "\n",
    "<!-- We will let $\\boldsymbol{\\beta}^*$ denote the anonymous user's hidden but true parameter that the AI will want to learn.  -->\n",
    "\n",
    "So, to travel by mode $i$ along edge $e^{j}_{i} = (d^{j}_{i},p^{j}_{i},t^{j}_{i}) \\in E_i$ with distance $d^{j}_{i}$, price $p^{j}_{i}$, and time $t^{j}_{i}$, a user will weigh their preference $\\boldsymbol{\\beta}$ as a linear combination:\n",
    "\n",
    "$$\n",
    "{\\boldsymbol{\\beta}}^{\\intercal} e^{j}_{i} = \\beta_0 d^{j}_{i} + \\beta_1 p^{j}_{i} + \\beta_2 t^{j}_{i}, \\quad \\boldsymbol{\\beta} = \n",
    "\\begin{bmatrix}\n",
    "\\beta_0\\\\\n",
    "\\beta_1\\\\\n",
    "\\beta_2\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boltzmann_policy(all_paths,beta):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      all_paths (type `list(tuple)`): \n",
    "          The list of valid paths being considered by the user.\n",
    "          \n",
    "          Example: [[(7, 0, 2), (0, 3, 0), (3, 2, 1), (2, 1, 3), (1, 5, 2)],\n",
    "                    [(7, 0, 3), (0, 3, 0), (3, 2, 1), (2, 1, 3), (1, 5, 2)],\n",
    "                    [(7, 0, 4), (0, 3, 0), (3, 2, 1), (2, 1, 3), (1, 5, 2)]]\n",
    "          \n",
    "      beta (type `numpy.array`):\n",
    "          The vector of the user's hidden preference parameter.\n",
    "          \n",
    "          Example: np.array([0.05, 0.3, 0.65])\n",
    "\n",
    "    Returns:\n",
    "      tuple(all_paths, action_probabilities)\n",
    "      \n",
    "      all_paths (type `list`): \n",
    "          Same as the input.\n",
    "          \n",
    "      path_probabilities (type `list(numeric)` or `numpy.array`): \n",
    "          The probabilities (pmf) of selecting the corresponding path as calculated\n",
    "          according to Boltzmann rationality. Note that this must be normalized\n",
    "          to sum to 1, so that it is a proper distribution.\n",
    "          \n",
    "          Example: [.5,.2,.3] or np.array([.5,.2,.3])\n",
    "    \"\"\"    \n",
    "    all_costs =[]\n",
    "    for path in all_paths:\n",
    "        path_properties = np.array([task.look_up_cost(edge) for edge in path])\n",
    "        \n",
    "        raise NotImplementedError(\"User model has not been implemented\")\n",
    "#TODO: Replace _ with the Boltzmann user model \n",
    "        path_costs = [_ for p in path_properties]\n",
    "        \n",
    "        journey_cost = np.sum(path_costs)\n",
    "        all_costs.append(journey_cost)\n",
    "\n",
    "    cost_array = np.array(all_costs)\n",
    "    raise NotImplementedError(\"Output not yet a proper distribution\")\n",
    "#TODO: make the cost array a proper distribution here \n",
    "    path_probabilities = None \n",
    "        \n",
    "    return all_paths, path_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to test if your function behaves correctly\n",
    "test_params = np.array([1,1,1])\n",
    "test_paths = [[(7, 0, 2), (0, 3, 0), (3, 2, 1), (2, 1, 3), (1, 5, 2)],\n",
    "              [(7, 0, 2), (0, 3, 0), (3, 2, 1), (2, 1, 3), (1, 5, 3)],\n",
    "              [(7, 0, 2), (0, 3, 0), (3, 2, 1), (2, 1, 3), (1, 5, 4)]]\n",
    "expected_probs = [0.3383749,0.33084034,0.33078477]\n",
    "paths, probs = boltzmann_policy(test_paths,test_params)\n",
    "\n",
    "assert paths == test_paths, f\"output actions {paths} don't match the input actions {test_paths}\"\n",
    "assert np.allclose(probs,expected_probs), f\"expected:{expected_probs}, got:{probs}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9LJahApAFvkP"
   },
   "source": [
    "## Task 2: Implement the user inference <a class=\"anchor\" id=\"task2\"></a>\n",
    "\n",
    "Every action the anonymous user makes informs the AI of their preference. As such, given the observed action the user takes, the AI should infer the parameters which motivated the observed decision. Under Boltzmann rationality, this means inferring that the observed action is most likely the one which minimizes cost (or, more formally, maximizes utility). Many statistical methods exist for inference, but for the purposes of this tutorial, we will use the Bayesian method of posterior update via the LaPlace Approximation.\n",
    "\n",
    "Assume that the parameters $\\boldsymbol{\\beta}$ come from a Gaussian distribution. We know from our (Boltzmann rationality) user model describing the likelihood distribution of any action $a_t$ observed at interaction time $t$. We can use these two distributions to find the posterior via the standard postorior update:\n",
    "\n",
    "\\begin{align*}\n",
    "\\boldsymbol{\\beta} &\\sim \\mathcal{N}_{k=3} (0,1)\\\\\n",
    "a_t|\\boldsymbol{\\beta} &\\propto -\\exp \\Big \\{ \\boldsymbol{\\beta} \\cdot \\text{Cost}(a_i) \\Big \\}\\\\\n",
    "p(\\boldsymbol{\\beta}|a_t) &\\propto p(a_t|\\boldsymbol{\\beta})p(\\boldsymbol{\\beta})\n",
    "\\end{align*}\n",
    "\n",
    "In the cell below, implement the equation for finding the posterior. Due to optimization for the minimum, we've put in a factor of -1 in the function output -- please leave that factor there.\n",
    "\n",
    "Hints: \n",
    "- To avoid messy constants and potential overflow, you can compute the log posterior instead.\n",
    "- Note that $\\boldsymbol{\\beta}$ is a 3 dimensional vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior_update(beta,e_properties):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      beta (type `numpy.array`): \n",
    "          The 3-dimensional parameter vector representing the user's hidden preferences.\n",
    "          \n",
    "      e_properties (type `numpy.array`):\n",
    "          The 3-dimensional parameter vector containing the properties of an edge.\n",
    "\n",
    "    Returns:\n",
    "      posterior (type `numeric`): \n",
    "          The calculated posterior/log-posterior used for inference on beta.\n",
    "    \"\"\" \n",
    "    raise NotImplementedError(\"Posterior update has not been implemented\")\n",
    "#TODO: implement the equations for prior and likelihood distributions\n",
    "    log_prior = None\n",
    "    log_likelihood = None\n",
    "    posterior = log_prior + log_likelihood\n",
    "    return -1 * posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the cell below to check your implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = posterior_update(np.array([1,1,1]), np.array([3,5,1]))\n",
    "expected_output = -4.7431844003859815\n",
    "assert np.isclose(expected_output,posterior) or \\\n",
    "    np.isclose(np.exp(expected_output),posterior), \\\n",
    "    f'\\n expected:{expected_output} or {np.exp(expected_output)} \\n \\\n",
    "    got: {posterior}' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no general analytical solution for the posterior when using a Boltmann distributed likelihood. However, approximation methods exist. Here, we will use the basic LaPlace approximation, which says that we can approximate $p(\\boldsymbol{\\beta}|a_t)$ with a Gaussian distribution centered at the mode $\\hat{\\boldsymbol{\\beta}}$ and with standard deviation $\\mathbf{H}$ evaluated at the mode:\n",
    "\n",
    "\\begin{align*}\n",
    "\\boldsymbol{\\beta}|a_t &\\sim \\mathcal{N}_{k=3}(\\hat{\\boldsymbol{\\beta}}, \\mathbf{H}), \\\\\n",
    "\\hat{\\boldsymbol{\\beta}} &= \\arg\\max_{\\boldsymbol{\\beta}} \\nabla_{\\boldsymbol{\\beta}} \\; p(\\boldsymbol{\\beta}|a_t) \\\\\n",
    "\\mathbf{H}^{-1} &= - \\nabla_{\\boldsymbol{\\beta}} \\nabla_{\\boldsymbol{\\beta}} \\; p(\\boldsymbol{\\beta}|a_t)|_{\\boldsymbol{\\beta} = \\hat{\\boldsymbol{\\beta}}} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "**Bonus excercise**: \n",
    "\n",
    "If you want, you can also implment your own Laplace approximation using the `scipy.optimize` function. If that's not the right adventure for you, feel free to skip ahead to the next task.\n",
    "\n",
    "Notes: \n",
    "- Since $\\boldsymbol{\\beta}$ is a 3-dimensional vector, you should use a 3-dimensional Gaussian LaPlace approximation, e.g. `scipy.stats.multivariate_normal()`\n",
    "- You'll find the following scipy documentation helpful: https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.OptimizeResult.html#scipy.optimize.OptimizeResult\n",
    "- Due to the nature of the exponential function, the optimizer may find a saddle point, i.e. a non-positive definite matrix. To use the Hessian as a covariance matrix, it'll need to be positive definite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_engine(e_properties,prev_distribution):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      e_properties (type `numpy.array`):\n",
    "          The 3-dimensional parameter vector containing the properties of an edge.\n",
    "      prev_distribution of type `scipy.stats.distribution`:\n",
    "          The distribution at time t-1; used in case optimization does not succeed.\n",
    "\n",
    "    Returns:\n",
    "      approx_posterior (type `scipy.stats.distribution`): \n",
    "          The Laplace approximate distribution of the posteior/log-posterior\n",
    "    \"\"\" \n",
    "    # step 2: Use Laplace Approx for posterior\n",
    "    init_mean = multivariate_normal(mean=np.zeros(3),cov=np.eye(3)).rvs(1) #np.zeros(3)\n",
    "    optim = minimize(posterior_update, init_mean,\n",
    "                     args=(e_properties,), method='BFGS')\n",
    "    \n",
    "    raise NotImplementedError(\"Inference has not been implemented\")\n",
    "# TODO: extract the relevant informaiton from the `optim` object\n",
    "    b_hat = None\n",
    "    H = None\n",
    "    \n",
    "    if optim.success:  \n",
    "# TODO: Use the extracted info for LaPlace approximated distribution\n",
    "        approx_posterior = None\n",
    "        return approx_posterior \n",
    "    \n",
    "    else:\n",
    "        return prev_distribution\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the following cell to test your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=12345)\n",
    "inf_output = inference_engine(np.array([1,3,43]))\n",
    "sample = inf_output.rvs(size=1)\n",
    "assert sample.shape[0] == 3\n",
    "\n",
    "expected_mean = [0.03999997, 0.12000005, 0.83999998]\n",
    "\n",
    "assert np.allclose(inf_output.mean,expected_mean)\n",
    "assert np.allclose(inf_output.cov,np.eye(3),atol=1e-6), \\\n",
    "        f\"{os.linesep}expected: {os.linesep} {np.eye(3)}, \\\n",
    "          {os.linesep}got:{os.linesep} {inf_output.cov}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have now completed Tasks 1 and 2!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) The AI-user interaction <a class=\"anchor\" id=\"intxn\"></a>\n",
    "\n",
    "We are now ready to put every thing together and model the interaction between the human and the AI assistant. Here, the aim of the sequential interaction is to build a journey itinerary -- an ordered list of edges $[e^1_i,...e^m_j]$ -- between locations 7 and 5.\n",
    "\n",
    "Recall that the interaction consists of the following steps:\n",
    "\n",
    "1. Both agents observe the task state\n",
    "2. AI makes a recommendation\n",
    "3. User considers that recommendation \n",
    "4. User proposes an action based on the recommendation\n",
    "5. AI updates the inferred user parameters $\\boldsymbol{\\beta}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9LJahApAFvkP"
   },
   "source": [
    "## Task 3: Implement the AI-user interaction <a class=\"anchor\" id=\"task3\"></a>\n",
    "\n",
    "Run the below cell to initialize plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [11, 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize your agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the introduciton section, you initialized the task already. Next we need to intialize the two agents working on the task.\n",
    "\n",
    "\n",
    "Run the following cell to initialize your anonymous user. Feel free to rename them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anonUser = crUser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll initialize the AI assistant you created.\n",
    "\n",
    "Run the following cell to inject your implementations into the AI-assistant. If you implemented your own LaPlace approximation function, be sure to uncomment the second `model_overwrites` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_overwrites = dict(policy_fn = boltzmann_policy,\n",
    "                        posterior_fn = posterior)\n",
    "\n",
    "# Uncomment the below dictionary if you implemented your own Laplace Approximation function\n",
    "# model_overwrites = dict(policy_fn = boltzmann_policy,\n",
    "#                         posterior_fn = posterior,\n",
    "#                         inference = inference_engine)\n",
    "\n",
    "simulatedUser = UserModel(**model_overwrites)\n",
    "\n",
    "ai_overwrites = dict(user_model = simulatedUser)\n",
    "myAssistant = Assistant(**ai_overwrites)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the two agents initialized, it's time to get them to work together on the task. You'll find below a skeleton of the interaction loop. Fill it out correctly and you'll see the affect of the two agent's working on the task!\n",
    "\n",
    "You'll find the following methods helpful:\n",
    "- Task\n",
    "    - `task.step(user_action=None)` or `task.step(ai_action=None)`:\n",
    "        - Input: action list from an agent\n",
    "        - Output: None\n",
    "        - Side effects: `World` object updates states based on agent action provided\n",
    "- Agents\n",
    "    - `agent.observe(task_state)`: \n",
    "        - Input: `World` object\n",
    "        - Output: None\n",
    "        - Side effects: `World` object state is store into agent\n",
    "    - `agent.take_action(task_state)`:\n",
    "        - Input: `World` object\n",
    "        - Output: action list\n",
    "        - Side effects: None\n",
    "    - `agent.update_belief()`: Usually this method is rolled up within the `observe()` method, but here we've pulled it out for explicit illustrative purposes.\n",
    "        - Input: None\n",
    "        - Output: None\n",
    "        - Side effects: agent performs posterior inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anonUser = crUser()\n",
    "myAssistant = Assistant()\n",
    "\n",
    "task.reset()\n",
    "while not task.is_solved():\n",
    "\n",
    "    # Step 0) Both agents must observe the current state of the task\n",
    "\n",
    "\n",
    "    # Step 1a) The assistant gives a recommendation\n",
    "    \n",
    "    # Step 1b) The task is updated with the recommendation\n",
    "    \n",
    "    # Show AI action\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(task.display_path())\n",
    "    plt.pause(.5)\n",
    "\n",
    "    # Step 2) The user observes the action taken by the AI\n",
    "    \n",
    "    # Step 3a) The user proposes counter journey or agrees to the recommendation\n",
    "\n",
    "    # Step 3b) The task is updated with the user feedback\n",
    "    \n",
    "    \n",
    "    # Show both AI and user actions\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(task.display_path())\n",
    "    plt.pause(0.5)\n",
    "    \n",
    "    \n",
    "print(\"Congratulations! You have successfully solved the task.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method comparison *(Under development)* <a class=\"anchor\" id=\"analysis\"></a>\n",
    "\n",
    "Is this user model based strategy effective? The cell below runs 100 tests comparing how long it takes (as measured by the number of interactions) the AI and user must take to solve the task together.\n",
    "\n",
    "- \"user_model\" is the strategy you've implemented above\n",
    "- \"global_min\" does not learn a user model but only searches for the observed minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_runs = 100\n",
    "count_history = dict(user_model = [],\n",
    "                     global_min = [],\n",
    "                     random = [])\n",
    "for i in range(num_of_runs):\n",
    "    \n",
    "    total_interactions = run_task()\n",
    "    count_history[\"user_model\"].append(total_interactions)\n",
    "    \n",
    "    total_interactions = run_task(\"global_min\")\n",
    "    count_history[\"global_min\"].append(total_interactions)\n",
    "    \n",
    "    if i%100 == 1:\n",
    "        print(\"Progress: \",round(i/num_of_runs,3))\n",
    "    \n",
    "print({mtd: np.mean(c) for mtd,c in count_history.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your results should show that the \"global_min\" strategy is a less effective cooperation strategy, showing that having a model of the user can help with optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
